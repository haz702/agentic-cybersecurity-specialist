{"id":"CVE-2023-29374","publishedDate":"2023-04-05T02:15:37.340","cvssV3":9.8,"description":"In LangChain through 0.0.131, the LLMMathChain chain allows prompt injection attacks that can execute arbitrary code via the Python exec method.","references":["https://github.com/hwchase17/langchain/issues/1026","https://github.com/hwchase17/langchain/issues/814","https://github.com/hwchase17/langchain/pull/1119","https://twitter.com/rharang/status/1641899743608463365/photo/1","https://github.com/hwchase17/langchain/issues/1026","https://github.com/hwchase17/langchain/issues/814","https://github.com/hwchase17/langchain/pull/1119","https://twitter.com/rharang/status/1641899743608463365/photo/1"]}